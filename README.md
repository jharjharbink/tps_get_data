# ğŸ“Š TPS Data Architecture

Architecture data en 4 couches pour centraliser et analyser les donnÃ©es comptables de 3500+ dossiers clients.

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![MySQL](https://img.shields.io/badge/MySQL-8.0%2B-blue.svg)](https://www.mysql.com/)
[![Bash](https://img.shields.io/badge/Bash-5.0%2B-green.svg)](https://www.gnu.org/software/bash/)

---

## ğŸ¯ Vue d'ensemble

Pipeline ETL 4 couches pour centraliser les donnÃ©es de 3500+ dossiers comptables depuis ACD, Pennylane et DIA Valoxy.

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAW (copies brutes)                   ğŸ”§ EN COURS          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”œâ”€â”€ raw_dia         : DonnÃ©es cabinet (temps, exercices)  â”‚
â”‚  â”œâ”€â”€ raw_acd         : 6 tables centralisÃ©es (3500 bases)  â”‚
â”‚  â””â”€â”€ raw_pennylane   : Export Redshift                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â¬‡
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TRANSFORM (normalisation)             âš ï¸ NE PAS MODIFIER   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â””â”€â”€ transform_compta                                       â”‚
â”‚       â”œâ”€â”€ dossiers_acd, dossiers_pennylane                 â”‚
â”‚       â”œâ”€â”€ ecritures_mensuelles (C*/F* agrÃ©gÃ©s)             â”‚
â”‚       â”œâ”€â”€ ecritures_tiers_detaillees (401/411)             â”‚
â”‚       â””â”€â”€ exercices, temps_collaborateurs                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â¬‡
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MDM (rÃ©fÃ©rentiel maÃ®tre)              âš ï¸ NE PAS MODIFIER   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â””â”€â”€ mdm                                                    â”‚
â”‚       â”œâ”€â”€ dossiers (jointure SIREN)                        â”‚
â”‚       â”œâ”€â”€ collaborateurs, contacts                         â”‚
â”‚       â””â”€â”€ mapping_comptes_services                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â¬‡
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MART (vues mÃ©tier)                    âš ï¸ NE PAS MODIFIER   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”œâ”€â”€ mart_pilotage_cabinet    : Directeurs, comptables    â”‚
â”‚  â”œâ”€â”€ mart_controle_gestion    : ContrÃ´leurs de gestion    â”‚
â”‚  â””â”€â”€ mart_production_client   : Clients (holdings)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Sources de donnÃ©es

- **ACD/DIA** : 3500+ bases MySQL `compta_*` (serveur distant)
- **Pennylane** : Export Redshift (nouveau logiciel comptable)
- **DIA Valoxy** : Base locale pour donnÃ©es cabinet

---

## âš ï¸ RÃ¨gle importante

**â›” NE PAS TOUCHER AUX COUCHES TRANSFORM / MDM / MART**

Actuellement en phase de **validation de la couche RAW** :
- Import ACD centralisÃ© (raw_acd) en cours de test
- Import Pennylane en cours de validation
- Les couches supÃ©rieures ne doivent pas Ãªtre modifiÃ©es tant que RAW n'est pas stable

---

## ğŸš€ DÃ©marrage rapide

### PrÃ©requis

```bash
# MySQL 8.0+
mysql --version

# Bash 5.0+
bash --version

# Outils requis
which mysqldump
which xargs
```

### Configuration

1. **Copier le fichier de configuration :**

```bash
cp bash/config.sh.example bash/config.sh
```

2. **Ã‰diter bash/config.sh avec vos credentials :**

```bash
# Connexion ACD (serveur distant)
ACD_HOST="<IP_SERVEUR>"
ACD_PORT=3306
ACD_USER="<USER>"
ACD_PASS="<PASSWORD>"

# Connexion MySQL locale
LOCAL_USER="root"
LOCAL_PASS="<PASSWORD>"
```

âš ï¸ **Ce fichier est dans .gitignore** (ne jamais le commiter)

### Installation

```bash
# 1. CrÃ©er les schÃ©mas et tables
./run_pipeline.sh --init-only

# 2. Importer les donnÃ©es RAW (premiÃ¨re fois)
./run_pipeline.sh --data-only --acd-full

# 3. VÃ©rifier l'import
mysql -u root -p -e "SELECT COUNT(DISTINCT dossier_code) FROM raw_acd.histo_ligne_ecriture"
```

---

## ğŸ“š Documentation

- **[COMMANDS.md](COMMANDS.md)** - Guide complet de toutes les commandes
- **[README_raw_acd.md](README_raw_acd.md)** - Documentation dÃ©taillÃ©e de l'import ACD
- **[claude.md](claude.md)** - Documentation projet et vision stratÃ©gique

---

## ğŸ”§ Commandes principales

### Import RAW complet

```bash
# Import full (TRUNCATE + rÃ©import 3500 bases)
./run_pipeline.sh --acd-full
# DurÃ©e: ~60-90 minutes

# Import incrÃ©mental (nouveautÃ©s uniquement)
./run_pipeline.sh --acd-incremental
# DurÃ©e: ~20-30 minutes
```

### Import RAW uniquement (sans TRANSFORM/MDM/MART)

```bash
# Import RAW avec ACD full
./run_pipeline.sh --data-only --acd-full

# Import RAW avec ACD incrÃ©mental
./run_pipeline.sh --data-only --acd-incremental
```

### Import ACD spÃ©cifique

```bash
# Import complet (TRUNCATE + 3500 bases)
bash bash/raw/02_import_raw_compta.sh --full

# Import incrÃ©mental (depuis last_sync_date)
bash bash/raw/02_import_raw_compta.sh --incremental

# Import depuis une date spÃ©cifique
bash bash/raw/02_import_raw_compta.sh --since "01/01/2025 00:00:00"
```

### Nettoyage

```bash
# Supprimer TOUTES les bases
bash bash/util/clean_all.sh

# Vider uniquement raw_acd
bash bash/raw/02c_cleanup_acd.sh --full

# Afficher les statistiques
bash bash/raw/02c_cleanup_acd.sh --stats
```

### Benchmark

```bash
# Tester les performances d'import sur 10 bases
bash bash/util/benchmark_import_acd.sh
```

**RÃ©sultat** : âœ… MÃ©thode 1 (INSERT SELECT sans batching) est la plus rapide - **implÃ©mentÃ©e dans le script actuel**

---

## ğŸ‘¨â€ğŸ’» Guide Admin SystÃ¨me - Cas d'usage

### ğŸš€ PremiÃ¨re installation

```bash
# 1. Cloner le projet
git clone https://github.com/jharjharbink/tps_get_data.git
cd tps_get_data

# 2. Configurer les credentials (crÃ©er bash/config.sh)
cp bash/config.sh.example bash/config.sh
nano bash/config.sh  # Adapter les credentials MySQL et ACD

# 3. CrÃ©er les schÃ©mas et tables
./run_pipeline.sh --init-only

# 4. Premier import complet
./run_pipeline.sh --data-only --acd-full
```

**DurÃ©e estimÃ©e** : ~4-6h pour 3500 bases ACD

---

### ğŸ”„ Import quotidien automatique (cron)

**Recommandation** : Import incrÃ©mental tous les jours Ã  2h00

```bash
# Ajouter au crontab
crontab -e

# Ligne Ã  ajouter :
0 2 * * * cd /chemin/vers/tps_get_data && ./run_pipeline.sh --skip-init --acd-incremental >> logs/cron.log 2>&1
```

**Avantages** :
- âœ… Rapide (quelques minutes au lieu de 4-6h)
- âœ… Capture uniquement les nouveautÃ©s depuis `last_sync_date`
- âœ… Pas de TRUNCATE, utilise `ON DUPLICATE KEY UPDATE`

**Limitations** :
- âš ï¸ Les modifications d'Ã©critures existantes ne sont pas capturÃ©es (seules les nouvelles)
- ğŸ’¡ Solution : Import `--full` hebdomadaire le dimanche

---

### ğŸ”§ RÃ©import complet hebdomadaire

**Recommandation** : Import complet tous les dimanches Ã  1h00

```bash
# Cron hebdomadaire
0 1 * * 0 cd /chemin/vers/tps_get_data && bash bash/util/clean_all.sh <<< "oui" && ./run_pipeline.sh --acd-full >> logs/cron_weekly.log 2>&1
```

**Cas d'usage** :
- Capturer les modifications d'Ã©critures existantes
- Nettoyer d'Ã©ventuelles incohÃ©rences
- Reconstruire les partitions de maniÃ¨re propre

---

### ğŸš¨ Gestion d'incidents

#### Incident 1 : Import plantÃ© Ã  mi-parcours

```bash
# VÃ©rifier l'Ã©tat
mysql -u root -p -e "SELECT * FROM raw_acd.sync_tracking;"

# Option A : Reprendre l'import (si tables OK)
./run_pipeline.sh --skip-init --acd-full

# Option B : Tout recrÃ©er (si corruption)
bash bash/util/clean_all.sh
./run_pipeline.sh --acd-full
```

---

#### Incident 2 : Base ACD source inaccessible

```bash
# Test de connexion
mysql -h $ACD_HOST -P $ACD_PORT -u $ACD_USER -p$ACD_PASS -e "SELECT 1"

# Import RAW uniquement depuis sources accessibles (DIA + Pennylane)
bash bash/raw/01_import_raw_dia.sh
bash bash/raw/03_import_raw_pennylane.sh

# Rebuild TRANSFORM/MDM/MART avec donnÃ©es existantes
./run_pipeline.sh --skip-raw
```

---

#### Incident 3 : DonnÃ©es corrompues dans TRANSFORM/MDM

```bash
# RecrÃ©er uniquement les couches analytiques (sans retoucher RAW)
./run_pipeline.sh --skip-raw --skip-init

# Ou rebuild spÃ©cifique
./run_pipeline.sh --transform-only
./run_pipeline.sh --mdm-only
```

---

### ğŸ› Debug et troubleshooting

#### VÃ©rifier l'Ã©tat des imports

```sql
-- Ã‰tat de raw_acd
SELECT
    table_name,
    FORMAT(rows_count, 0) as lignes,
    last_sync_type as mode,
    DATE_FORMAT(last_sync_date, '%Y-%m-%d %H:%i') as derniere_synchro,
    last_status,
    last_duration_sec
FROM raw_acd.sync_tracking
ORDER BY table_name;

-- Nombre de dossiers centralisÃ©s
SELECT COUNT(DISTINCT dossier_code) FROM raw_acd.histo_ligne_ecriture;

-- VolumÃ©trie par partition
SELECT
    PARTITION_NAME,
    TABLE_ROWS,
    CONCAT(ROUND(DATA_LENGTH / 1024 / 1024, 2), ' MB') AS taille
FROM information_schema.PARTITIONS
WHERE TABLE_SCHEMA = 'raw_acd'
  AND TABLE_NAME = 'histo_ligne_ecriture'
ORDER BY PARTITION_ORDINAL_POSITION;
```

---

#### Logs en temps rÃ©el

```bash
# Suivre l'import en cours
tail -f logs/pipeline_*.log

# Filtrer les erreurs
grep -i "error\|erreur" logs/pipeline_*.log

# Nombre de bases importÃ©es
grep -c "OK: compta_" logs/pipeline_*.log
```

---

### ğŸ’¾ Backup et restauration

#### Backup avant grosse opÃ©ration

```bash
# Backup complet (RAW + TRANSFORM + MDM)
mysqldump \
    --single-transaction \
    --routines --triggers --events \
    raw_acd raw_dia raw_pennylane \
    transform_compta \
    mdm \
    > backup_full_$(date +%Y%m%d_%H%M%S).sql

# Compression
gzip backup_full_*.sql
```

---

#### Backup incrÃ©mental (raw_acd uniquement)

```bash
# Sauvegarde quotidienne de raw_acd
mysqldump --single-transaction raw_acd > backup_raw_acd_$(date +%Y%m%d).sql
gzip backup_raw_acd_*.sql

# Rotation : conserver 7 jours
find . -name "backup_raw_acd_*.sql.gz" -mtime +7 -delete
```

---

#### Restauration

```bash
# Restaurer depuis backup
gunzip -c backup_full_20250124.sql.gz | mysql -u root -p

# VÃ©rifier la restauration
mysql -e "SELECT * FROM raw_acd.sync_tracking;"
```

---

### ğŸ§ª Tests et validation

#### Test sur Ã©chantillon (20 bases)

```bash
# CrÃ©er branche de test
git checkout -b test/validation-import

# Modifier temporairement 02_import_raw_compta.sh
# (ajouter | head -20 aprÃ¨s ${ELIGIBLE_DATABASES[@]})

# Lancer le test
./run_pipeline.sh --init-only
bash bash/raw/02_import_raw_compta.sh --full

# VÃ©rifier rÃ©sultat
mysql -e "SELECT COUNT(DISTINCT dossier_code) FROM raw_acd.histo_ligne_ecriture;"
# Devrait retourner : 20

# Revenir Ã  main aprÃ¨s validation
git checkout main
```

---

### ğŸ“Š Monitoring et alertes

#### Script de monitoring (monitoring.sh)

```bash
#!/bin/bash
# VÃ©rifier l'Ã©tat du dernier import

LAST_STATUS=$(mysql -N -e "SELECT last_status FROM raw_acd.sync_tracking LIMIT 1;")
LAST_DURATION=$(mysql -N -e "SELECT last_duration_sec FROM raw_acd.sync_tracking LIMIT 1;")

if [ "$LAST_STATUS" != "success" ]; then
    echo "âŒ ALERTE : Dernier import en Ã©chec"
    # Envoyer notification (email, Slack, etc.)
    exit 1
fi

if [ "$LAST_DURATION" -gt 28800 ]; then  # Plus de 8h
    echo "âš ï¸  WARNING : Import anormalement long (${LAST_DURATION}s)"
fi

echo "âœ… Import OK (durÃ©e: ${LAST_DURATION}s)"
```

---

### ğŸ” Optimisations

#### Optimiser MySQL pour imports massifs

```sql
-- Avant import complet
SET GLOBAL innodb_buffer_pool_size = 8GB;
SET GLOBAL innodb_log_file_size = 512MB;
SET GLOBAL innodb_flush_log_at_trx_commit = 2;

-- AprÃ¨s import
SET GLOBAL innodb_flush_log_at_trx_commit = 1;
```

---

#### Surveiller l'espace disque

```bash
# VÃ©rifier avant import
df -h /var/lib/mysql

# Taille des bases
mysql -e "
SELECT
    table_schema AS 'Base',
    CONCAT(ROUND(SUM(data_length + index_length) / 1024 / 1024 / 1024, 2), ' GB') AS 'Taille'
FROM information_schema.tables
WHERE table_schema IN ('raw_acd', 'raw_dia', 'raw_pennylane', 'transform_compta', 'mdm')
GROUP BY table_schema
ORDER BY SUM(data_length + index_length) DESC;
"
```

---

## ğŸ“ Structure du projet

```
tps_get_data/
â”œâ”€â”€ run_pipeline.sh              # ğŸ¯ Orchestrateur principal
â”œâ”€â”€ bash/
â”‚   â”œâ”€â”€ config.sh                # Configuration (gitignored)
â”‚   â”œâ”€â”€ logging.sh               # Fonctions log uniformes
â”‚   â”œâ”€â”€ raw/                     # ğŸ”§ Scripts d'import RAW (FOCUS ACTUEL)
â”‚   â”‚   â”œâ”€â”€ 01_import_raw_dia.sh
â”‚   â”‚   â”œâ”€â”€ 02_import_raw_compta.sh       # Import ACD centralisÃ© â­
â”‚   â”‚   â”œâ”€â”€ 02b_import_incremental_acd.sh
â”‚   â”‚   â”œâ”€â”€ 02c_cleanup_acd.sh
â”‚   â”‚   â”œâ”€â”€ 03_import_raw_pennylane.sh
â”‚   â”‚   â”œâ”€â”€ 03b_cleanup_pennylane.sh
â”‚   â”‚   â””â”€â”€ run_all_raw.sh
â”‚   â”œâ”€â”€ transform/               # âš ï¸ NE PAS MODIFIER
â”‚   â”œâ”€â”€ mdm/                     # âš ï¸ NE PAS MODIFIER
â”‚   â”œâ”€â”€ mart/                    # âš ï¸ NE PAS MODIFIER
â”‚   â””â”€â”€ util/
â”‚       â”œâ”€â”€ clean_all.sh         # Suppression complÃ¨te
â”‚       â””â”€â”€ benchmark_import_acd.sh  # Benchmark performance
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ 01_create_schemas.sql
â”‚   â”œâ”€â”€ 02b_raw_acd_tables.sql   # Tables raw_acd (partitionnement) â­
â”‚   â”œâ”€â”€ 02_raw_pennylane_tables.sql
â”‚   â”œâ”€â”€ 03_transform_tables.sql  # âš ï¸ NE PAS MODIFIER
â”‚   â”œâ”€â”€ 04_mdm_tables.sql        # âš ï¸ NE PAS MODIFIER
â”‚   â”œâ”€â”€ 05_mart_views.sql        # âš ï¸ NE PAS MODIFIER
â”‚   â””â”€â”€ 06-08_procedures_*.sql   # âš ï¸ NE PAS MODIFIER
â”œâ”€â”€ logs/                        # Logs des exÃ©cutions
â”œâ”€â”€ COMMANDS.md                  # Guide des commandes
â”œâ”€â”€ README_raw_acd.md            # Doc import ACD
â””â”€â”€ claude.md                    # Doc projet complÃ¨te
```

---

## ğŸ” RequÃªtes utiles

### Statistiques raw_acd

```sql
-- Derniers imports
SELECT * FROM raw_acd.sync_tracking;

-- Nombre de dossiers centralisÃ©s
SELECT COUNT(DISTINCT dossier_code) FROM raw_acd.histo_ligne_ecriture;

-- VolumÃ©trie par table
SELECT
    table_name,
    FORMAT(rows_count, 0) as nb_lignes,
    last_sync_type as mode,
    DATE_FORMAT(last_sync_date, '%Y-%m-%d %H:%i') as derniere_synchro
FROM raw_acd.sync_tracking
ORDER BY table_name;

-- Taille des donnÃ©es
SELECT
    table_name,
    CONCAT(ROUND(data_length / 1024 / 1024, 2), ' MB') AS donnees,
    CONCAT(ROUND(index_length / 1024 / 1024, 2), ' MB') AS index,
    CONCAT(ROUND((data_length + index_length) / 1024 / 1024, 2), ' MB') AS total
FROM information_schema.tables
WHERE table_schema = 'raw_acd'
ORDER BY (data_length + index_length) DESC;
```

### VÃ©rifier la qualitÃ© des donnÃ©es

```sql
-- Dossiers avec le plus d'Ã©critures
SELECT
    dossier_code,
    COUNT(*) as nb_ecritures
FROM raw_acd.histo_ligne_ecriture
GROUP BY dossier_code
ORDER BY nb_ecritures DESC
LIMIT 20;

-- Distribution par annÃ©e
SELECT
    HE_ANNEE as annee,
    COUNT(*) as nb_lignes
FROM raw_acd.histo_ligne_ecriture
GROUP BY HE_ANNEE
ORDER BY HE_ANNEE;
```

---

## ğŸ¯ Focus actuel : Couche RAW

### Import ACD centralisÃ© (raw_acd)

**ProblÃ©matique** : 3500 bases `compta_*` avec 50+ tables chacune â†’ stockage Ã©norme

**Solution** : Import sÃ©lectif de 6 tables dans `raw_acd` centralisÃ©e

**Tables importÃ©es** :
1. `histo_ligne_ecriture` - Lignes Ã©critures historiques (partitionnÃ© par annÃ©e)
2. `histo_ecriture` - En-tÃªtes Ã©critures historiques
3. `ligne_ecriture` - Lignes Ã©critures courantes
4. `ecriture` - En-tÃªtes Ã©critures courantes
5. `compte` - Plan comptable
6. `journal` - Journaux

**MÃ©canisme** :
- Mode `--full` : TRUNCATE + rÃ©import complet
- Mode `--incremental` : Import avec filtre `WHERE date > last_sync_date` + `ON DUPLICATE KEY UPDATE`
- Tracking via `sync_tracking` (last_sync_date, rows_count, duration)

**Performance** :
- Import sÃ©quentiel (source ACD 1 CPU - pas de parallÃ©lisme)
- Compression MySQL : `--compress`
- **âš¡ OptimisÃ©** : MÃ©thode 1 du benchmark (INSERT SELECT sans batching)
- **âš¡ last_sync_date** : rÃ©cupÃ©rÃ© 1x au lieu de 3500x en mode incrÃ©mental
- Estimation : ~4-6h pour 3500 bases

---

## âš¡ Automatisation (cron)

### Import quotidien Ã  2h00

```cron
0 2 * * * cd /path/to/tps_get_data && ./run_pipeline.sh --acd-incremental 2>&1 | logger -t data_pipeline
```

### Import complet hebdomadaire (dimanche 1h00)

```cron
0 1 * * 0 cd /path/to/tps_get_data && ./run_pipeline.sh --acd-full 2>&1 | logger -t data_pipeline
```

---

## ğŸ› DÃ©pannage

### ProblÃ¨me : Connexion ACD Ã©choue

```bash
# Tester la connexion
mysql -h <ACD_HOST> -P <ACD_PORT> -u <ACD_USER> -p<ACD_PASS> -e "SELECT 1"
```

### ProblÃ¨me : Import trÃ¨s lent

```bash
# VÃ©rifier le nombre de bases Ã  traiter
mysql -h <ACD_HOST> -u <ACD_USER> -p -e "
    SELECT COUNT(*) FROM information_schema.schemata
    WHERE schema_name LIKE 'compta_%'
"

# Lancer le benchmark pour comparer les mÃ©thodes
bash bash/util/benchmark_import_acd.sh
```

### ProblÃ¨me : DonnÃ©es manquantes

```bash
# VÃ©rifier sync_tracking
mysql -u root -p raw_acd -e "SELECT * FROM sync_tracking"

# Relancer import complet
bash bash/raw/02_import_raw_compta.sh --full
```

---

## ğŸ“Š Logs

Les logs sont disponibles dans :
```
logs/pipeline_YYYYMMDD_HHMMSS.log
```

Rotation automatique : conservation de 30 jours

---

## ğŸ¯ Roadmap

### Phase 1 : Stabilisation RAW (EN COURS)
- âœ… Import ACD centralisÃ© (raw_acd)
- âœ… Optimisation performances (benchmark MÃ©thode 1 appliquÃ©)
- ğŸ”„ Validation import incrÃ©mental en production
- â³ Tests sur 3500 bases

### Phase 2 : Adaptation TRANSFORM
1. Adapter les procÃ©dures pour utiliser raw_acd
2. Tester les agrÃ©gations ecritures_mensuelles
3. Valider la qualitÃ© des donnÃ©es transformÃ©es

### Phase 3 : Enrichissement MDM
1. DÃ©duplication SIREN
2. Jointure multi-sources (ACD â†” Pennylane â†” Silae)
3. API backend pour synchronisation

### Phase 4 : Vues MART par profil utilisateur
- MART Comptables (directeurs, comptables)
- MART ContrÃ´le de gestion (interne/externe)
- MART Production client (holdings)

---

## ğŸ“ Support

- **Documentation** : Voir [COMMANDS.md](COMMANDS.md) et [README_raw_acd.md](README_raw_acd.md)
- **Logs** : `logs/pipeline_YYYYMMDD_HHMMSS.log`
- **Issues** : [GitHub Issues](https://github.com/jharjharbink/tps_get_data/issues)

---

## ğŸ“ License

MIT License - voir [LICENSE](LICENSE) pour plus de dÃ©tails

---

**âš ï¸ Focus actuel : VALIDATION DE LA COUCHE RAW UNIQUEMENT**
