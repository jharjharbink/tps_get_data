# üì¶ RAW_ACD - Import centralis√© des donn√©es comptables ACD

## üéØ Vue d'ensemble

Ce module remplace l'import des bases `compta_*` compl√®tes par un **import s√©lectif** de 6 tables dans une base centralis√©e `raw_acd`.

### Avant / Apr√®s

**AVANT :**
```
‚îú‚îÄ‚îÄ compta_00001 (base compl√®te copi√©e)
‚îú‚îÄ‚îÄ compta_00002 (base compl√®te copi√©e)
‚îî‚îÄ‚îÄ ... (3500+ bases)
```
‚ùå Import lourd et lent
‚ùå Stockage multipli√©
‚ùå Impossible √† requ√™ter efficacement

**APR√àS :**
```
raw_acd (base unique centralis√©e)
‚îú‚îÄ‚îÄ histo_ligne_ecriture (avec colonne dossier_code)
‚îú‚îÄ‚îÄ histo_ecriture
‚îú‚îÄ‚îÄ ligne_ecriture
‚îú‚îÄ‚îÄ ecriture
‚îú‚îÄ‚îÄ compte
‚îî‚îÄ‚îÄ journal
```
‚úÖ Import s√©lectif de 6 tables uniquement
‚úÖ Requ√™tes SQL simples
‚úÖ Compatible Power BI
‚úÖ Mode incr√©mental pour mises √† jour rapides

---

## üìÅ Fichiers cr√©√©s/modifi√©s

### Nouveaux fichiers SQL
- **`sql/02b_raw_acd_tables.sql`** : Cr√©ation des 6 tables avec partitionnement par ann√©e

### Nouveaux scripts Bash
- **`bash/raw/02_import_raw_compta.sh`** : Import principal (modes --full / --incremental)
- **`bash/raw/02b_import_incremental_acd.sh`** : Wrapper pour import quotidien
- **`bash/raw/02c_cleanup_acd.sh`** : Nettoyage et maintenance

### Fichiers modifi√©s
- **`sql/01_create_schemas.sql`** : Ajout du sch√©ma `raw_acd`
- **`bash/raw/run_all_raw.sh`** : Support options `--acd-full` / `--acd-incremental`
- **`run_pipeline.sh`** : Int√©gration compl√®te dans le pipeline

---

## ‚öôÔ∏è Configuration requise

Ajoutez ces variables dans votre `bash/config.sh` local (non versionn√©) :

```bash
# ‚îÄ‚îÄ‚îÄ SERVEUR ACD (pour raw_acd) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
export ACD_HOST="192.168.20.24"
export ACD_PORT="3306"
export ACD_USER="root"
export ACD_PASS="admin-2019"

# ‚îÄ‚îÄ‚îÄ TABLES REQUISES POUR RAW_ACD ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
export REQUIRED_TABLES=(
    "histo_ligne_ecriture"
    "histo_ecriture"
    "ligne_ecriture"
    "ecriture"
    "compte"
    "journal"
)

# ‚îÄ‚îÄ‚îÄ BASES COMPTA_* √Ä EXCLURE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
export EXCLUDED_DATABASES=(
    "compta_000000"
    "compta_zz"
    "compta_gombertcOLD"
    "compta_gombertcold"
)
```

---

## üöÄ Installation

### 1. Cr√©er le sch√©ma et les tables

```bash
./run_pipeline.sh --init-only
```

Cela cr√©e automatiquement :
- Le sch√©ma `raw_acd`
- Les 6 tables avec partitionnement
- La table `sync_tracking` pour l'incr√©mental
- La vue unifi√©e `v_ligne_ecriture_unified`

---

## üìñ Utilisation

### Import initial (premi√®re fois)

```bash
# Via le pipeline complet
./run_pipeline.sh

# Ou seulement l'import raw_acd
bash bash/raw/02_import_raw_compta.sh --full
```

**Dur√©e estim√©e :** ~45-60 minutes pour 3500 bases (avec 3 jobs parall√®les)

**Ce qui est fait :**
1. V√©rification de la connexion ACD
2. R√©cup√©ration de toutes les bases `compta_*`
3. **V√©rification que les 6 tables requises existent** dans chaque base
4. Filtrage des bases √©ligibles
5. Import parall√®le (3 jobs) vers `raw_acd`
6. Ajout automatique de la colonne `dossier_code`

---

### Import incr√©mental (quotidien)

```bash
# Via le wrapper
bash bash/raw/02b_import_incremental_acd.sh

# Ou via le script principal
bash bash/raw/02_import_raw_compta.sh --incremental

# Ou via le pipeline complet
./run_pipeline.sh --acd-incremental
```

**Dur√©e estim√©e :** ~5-10 minutes (selon volume de modifications)

**Ce qui est fait :**
1. Lecture de `last_sync_date` depuis `sync_tracking`
2. Import uniquement des lignes modifi√©es (bas√© sur `HE_DATE_SAI` / `ECR_DATE_SAI`)
3. Utilisation de `ON DUPLICATE KEY UPDATE` pour √©viter les doublons
4. Mise √† jour automatique de `sync_tracking`

---

### Nettoyage et maintenance

```bash
# Afficher les statistiques
bash bash/raw/02c_cleanup_acd.sh --stats

# Supprimer un dossier sp√©cifique
bash bash/raw/02c_cleanup_acd.sh --dossier 00123

# Supprimer une ann√©e compl√®te
bash bash/raw/02c_cleanup_acd.sh --year 2020

# Supprimer avant une date
bash bash/raw/02c_cleanup_acd.sh --before 2022-01-01

# Optimiser les tables (r√©cup√©rer l'espace disque)
bash bash/raw/02c_cleanup_acd.sh --optimize

# Vider compl√®tement raw_acd (avec confirmation)
bash bash/raw/02c_cleanup_acd.sh --full
```

---

## üîß Options du pipeline

### Options g√©n√©rales

```bash
./run_pipeline.sh                    # Pipeline complet (mode full ACD)
./run_pipeline.sh --skip-raw         # Sans r√©import RAW
./run_pipeline.sh --init-only        # Cr√©er uniquement les sch√©mas
./run_pipeline.sh --data-only        # Importer uniquement les donn√©es
```

### Options sp√©cifiques ACD

```bash
./run_pipeline.sh --acd-full         # Import complet (TRUNCATE + r√©import)
./run_pipeline.sh --acd-incremental  # Import incr√©mental (nouveaut√©s uniquement)
```

### Combinaisons utiles

```bash
# Import quotidien rapide (incr√©mental)
./run_pipeline.sh --acd-incremental

# R√©initialisation compl√®te hebdomadaire
./run_pipeline.sh --acd-full

# Import donn√©es sans recr√©er les tables
./run_pipeline.sh --data-only --acd-incremental
```

---

## üìä Structure des tables

### Tables principales (avec partitionnement)

```sql
CREATE TABLE raw_acd.histo_ligne_ecriture (
    dossier_code VARCHAR(20),        -- '00123' (extrait de compta_00123)
    HLE_CODE BIGINT,
    HE_CODE BIGINT,
    CPT_CODE VARCHAR(32),
    HLE_CRE_ORG DECIMAL(18,2),
    HLE_DEB_ORG DECIMAL(18,2),
    HE_DATE_SAI DATE,                -- Pour incr√©mental
    HE_ANNEE SMALLINT,               -- Pour partitionnement
    HE_MOIS TINYINT,
    JNL_CODE VARCHAR(32),
    PRIMARY KEY (dossier_code, HLE_CODE, HE_ANNEE)
) PARTITION BY RANGE (HE_ANNEE);
```

### Table de tracking

```sql
SELECT * FROM raw_acd.sync_tracking;
```

| table_name | last_sync_date | last_sync_type | rows_count | last_status |
|------------|----------------|----------------|------------|-------------|
| histo_ligne_ecriture | 2025-11-23 02:00 | incremental | 12548732 | success |

### Vue unifi√©e

```sql
SELECT * FROM raw_acd.v_ligne_ecriture_unified
WHERE dossier_code = '00123'
AND annee = 2024;
```

---

## üîç Requ√™tes utiles

### Compter les dossiers centralis√©s

```sql
SELECT COUNT(DISTINCT dossier_code) as nb_dossiers
FROM raw_acd.histo_ligne_ecriture;
```

### Balance mensuelle pour un dossier

```sql
SELECT
    DATE_FORMAT(date_saisie, '%Y-%m-01') as period_month,
    CPT_CODE,
    SUM(debit) as debits,
    SUM(credit) as credits,
    SUM(debit - credit) as solde
FROM raw_acd.v_ligne_ecriture_unified
WHERE dossier_code = '00123'
  AND annee >= 2024
GROUP BY period_month, CPT_CODE;
```

### Top 10 des dossiers par volume

```sql
SELECT
    dossier_code,
    COUNT(*) as nb_ecritures,
    SUM(HLE_DEB_ORG + HLE_CRE_ORG) as volume_total
FROM raw_acd.histo_ligne_ecriture
GROUP BY dossier_code
ORDER BY nb_ecritures DESC
LIMIT 10;
```

---

## ‚ö° Performances

### Volumes estim√©s (3500 bases)

| Table | Lignes | Taille |
|-------|--------|--------|
| histo_ligne_ecriture | ~12M | ~1.5 GB |
| ligne_ecriture | ~2M | ~250 MB |
| histo_ecriture | ~4M | ~400 MB |
| ecriture | ~800K | ~80 MB |
| compte | ~150K | ~15 MB |
| journal | ~20K | ~2 MB |
| **TOTAL** | **~19M** | **~2.3 GB** |

### Temps d'ex√©cution

| Op√©ration | Dur√©e |
|-----------|-------|
| Import full (3500 bases, 3 jobs) | ~45-60 min |
| Import incr√©mental quotidien | ~5-10 min |
| Requ√™te balance mensuelle | < 5 sec |

### üî¨ Benchmark de performance

Pour mesurer les performances r√©elles sur votre environnement et choisir la meilleure m√©thode d'import :

```bash
bash bash/util/benchmark_import_acd.sh
```

Ce script teste **4 m√©thodes** sur 10 bases pour comparer les performances :

**M√©thodes test√©es :**
1. **M√©thode 1 : INSERT SELECT SANS batching** ‚≠ê **LA PLUS RAPIDE**
   - Requ√™te directe : `INSERT INTO raw_acd.table SELECT 'dossier', t.* FROM compta_*.table t`
   - 1 requ√™te par table (6 requ√™tes par base)
   - Moins d'overhead r√©seau
   - **‚úÖ IMPL√âMENT√âE dans le script actuel**

2. **M√©thode 2 : INSERT SELECT AVEC batching (toutes tables)**
   - Batching de 100k lignes pour les 6 tables
   - Plus de requ√™tes mais chunks plus petits
   - Peut √™tre utile pour tr√®s grosses tables

3. **M√©thode 3 : INSERT SELECT AVEC batching (√©critures seulement)**
   - Batching uniquement pour les 4 tables d'√©critures
   - compte/journal import√©s en 1 fois

4. **M√©thode 4 : DUMP COMPLET** (ancien script - r√©f√©rence)
   - Clone complet des bases avec mysqldump
   - ‚ùå Incompatible avec architecture raw_acd
   - Conserv√© pour r√©f√©rence historique

**R√©sultat du benchmark** :
- ‚úÖ **M√©thode 1 (INSERT SELECT sans batching) est la plus rapide**
- Le batching n'apporte pas d'am√©lioration pour les volumes actuels
- Code plus simple et maintenable

Le benchmark g√©n√®re un rapport d√©taill√© (`benchmark_results_YYYYMMDD_HHMMSS.txt`) avec :
- Tableau comparatif des temps d'ex√©cution
- Estimation pour 3500 bases
- Analyse comparative entre m√©thodes
- Recommandation finale

**Dur√©e du benchmark :** ~5-15 minutes selon les volumes

**‚ö° Optimisations appliqu√©es dans le script actuel** :
- ‚úÖ M√©thode 1 impl√©ment√©e (INSERT SELECT sans batching)
- ‚úÖ R√©cup√©ration `last_sync_date` UNE SEULE FOIS (au lieu de 3500+ requ√™tes en mode incr√©mental)
- ‚úÖ Requ√™tes SQL simplifi√©es et inline
- ‚úÖ Moins d'overhead et meilleure performance

---

## ‚úÖ V√©rifications

### V√©rifier que les 6 tables existent

Le script `02_import_raw_compta.sh` v√©rifie automatiquement que chaque base `compta_*` poss√®de les 6 tables requises. Les bases sans toutes les tables sont **automatiquement exclues** et un warning est affich√©.

### Logs d'exclusion

```
[2025-11-23 10:15:32] [WARNING] ‚ö†Ô∏è  Base compta_test ignor√©e : tables manquantes
[2025-11-23 10:15:33] [INFO] ‚ÑπÔ∏è  3452 bases √©ligibles trouv√©es (48 exclues)
```

---

## üõ†Ô∏è D√©pannage

### Erreur "raw_acd n'existe pas"

```bash
./run_pipeline.sh --init-only
```

### Import incr√©mental ne trouve rien

V√©rifier la derni√®re synchro :
```sql
SELECT * FROM raw_acd.sync_tracking;
```

### Bases manquent des tables requises

V√©rifier quelles tables existent :
```sql
SELECT table_name
FROM information_schema.tables
WHERE table_schema = 'compta_00123';
```

### Relancer un import complet

```bash
bash bash/raw/02_import_raw_compta.sh --full
```

---

## üîÆ Prochaines √©tapes (Next Steps)

Une fois `raw_acd` en place, vous pourrez :

1. **Supprimer les anciennes bases `compta_*` locales** (lib√©rer ~50GB+)
2. **Migrer vers transform_compta** : Utiliser `raw_acd` au lieu de boucler sur les bases
3. **Cr√©er des dashboards Power BI** : Requ√™tes directes sur `raw_acd`
4. **Automatiser avec cron** :
   ```bash
   # Import incr√©mental quotidien √† 2h00
   0 2 * * * /path/to/bash/raw/02b_import_incremental_acd.sh

   # Import complet hebdomadaire le dimanche √† 1h00
   0 1 * * 0 /path/to/bash/raw/02_import_raw_compta.sh --full
   ```

### ‚ö†Ô∏è Am√©liorations prioritaires

#### 1. ‚úÖ **Horodatage par base compta_* lors de l'import** (IMPL√âMENT√â)

**Probl√®me initial** : Si un import dure plusieurs heures pour 3500 bases, les bases ACD source peuvent √™tre modifi√©es pendant le traitement. La date `last_sync_date` dans `sync_tracking` √©tait mise √† jour **√† la fin** de tout l'import.

**‚úÖ Solution impl√©ment√©e** :
- Nouvelle table `sync_tracking_by_dossier` cr√©√©e (pr√©serve l'ancienne table `sync_tracking`)
- Enregistrement de la date d'import **par dossier** au fur et √† mesure
- Structure de la table :
  ```sql
  CREATE TABLE sync_tracking_by_dossier (
      table_name VARCHAR(50) NOT NULL,
      dossier_code VARCHAR(20) NOT NULL,
      last_sync_date DATETIME NOT NULL,
      last_sync_type ENUM('full', 'incremental', 'since'),
      last_status VARCHAR(20) DEFAULT 'success',
      rows_imported INT DEFAULT 0,
      PRIMARY KEY (table_name, dossier_code)
  );
  ```

**‚úÖ B√©n√©fices obtenus** :
- Import incr√©mental plus pr√©cis (par dossier)
- Tra√ßabilit√© exacte de chaque base
- Reprise possible apr√®s crash sans tout r√©importer
- Double tracking : global (`sync_tracking`) + granulaire (`sync_tracking_by_dossier`)

---

#### 2. ‚úÖ **Optimisation import incr√©mental : filtrer par dossiers d√©j√† import√©s** (IMPL√âMENT√â)

**Probl√®me initial** : En mode `--incremental`, le script traitait **tous les dossiers** trouv√©s sur le serveur ACD, m√™me ceux qui n'avaient jamais √©t√© import√©s auparavant.

**Cons√©quence** :
- Si un nouveau dossier `compta_99999` √©tait cr√©√© sur ACD, il aurait √©t√© import√© en mode incr√©mental
- Mais la date de r√©f√©rence aurait √©t√© `last_sync_date` globale ‚Üí risque de manquer des donn√©es historiques
- Pas de tra√ßabilit√© des nouveaux dossiers d√©tect√©s

**‚úÖ Solution impl√©ment√©e** :
```bash
# Dans 02_import_raw_compta.sh, apr√®s r√©cup√©ration des bases √©ligibles

if [ "$MODE" = "incremental" ]; then
    log "INFO" "Mode incr√©mental : filtrage des dossiers d√©j√† connus..."

    # R√©cup√©rer les dossiers d√©j√† import√©s (pr√©sents dans raw_acd)
    KNOWN_DOSSIERS=$($MYSQL $MYSQL_OPTS raw_acd -N -e "
        SELECT DISTINCT dossier_code
        FROM sync_tracking_by_dossier
    ")

    # D√©tecter les nouveaux dossiers
    NEW_DOSSIERS=()
    KNOWN_DOSSIERS_ARRAY=()

    for DB in "${ELIGIBLE_DATABASES[@]}"; do
        DOSSIER_CODE="${DB#compta_}"

        if echo "$KNOWN_DOSSIERS" | grep -qx "$DOSSIER_CODE"; then
            KNOWN_DOSSIERS_ARRAY+=("$DB")
        else
            NEW_DOSSIERS+=("$DB")
        fi
    done

    # Logger les nouveaux dossiers d√©tect√©s
    if [ ${#NEW_DOSSIERS[@]} -gt 0 ]; then
        log "WARNING" "üÜï ${#NEW_DOSSIERS[@]} nouveaux dossiers d√©tect√©s (non import√©s en incr√©mental) :"
        for NEW_DB in "${NEW_DOSSIERS[@]}"; do
            log "WARNING" "   - $NEW_DB (n√©cessite un import --full pour historique complet)"
        done
    fi

    # Utiliser uniquement les dossiers connus pour l'import incr√©mental
    ELIGIBLE_DATABASES=("${KNOWN_DOSSIERS_ARRAY[@]}")
    log "INFO" "Import incr√©mental limit√© √† ${#ELIGIBLE_DATABASES[@]} dossiers connus"
fi
```

**‚úÖ B√©n√©fices obtenus** :
- ‚úÖ **Performance** : Import incr√©mental plus rapide (ne traite que les dossiers d√©j√† connus)
- ‚úÖ **Tra√ßabilit√©** : Log WARNING avec liste des nouveaux dossiers d√©tect√©s
- ‚úÖ **S√©curit√©** : √âvite d'importer partiellement un nouveau dossier (risque de donn√©es manquantes)
- ‚úÖ **Workflow clair** :
  - Import `--incremental` = mise √† jour uniquement des dossiers existants
  - Import `--full` = ajout de nouveaux dossiers + mise √† jour compl√®te de tous les dossiers

**Exemple de log attendu** :
```
[2025-01-25 14:30:00] [INFO] ‚ÑπÔ∏è  Mode incr√©mental : filtrage des dossiers d√©j√† connus...
[2025-01-25 14:30:02] [WARNING] ‚ö†Ô∏è  üÜï 3 nouveaux dossiers d√©tect√©s (non import√©s en incr√©mental) :
[2025-01-25 14:30:02] [WARNING] ‚ö†Ô∏è     - compta_99999 (n√©cessite un import --full pour historique complet)
[2025-01-25 14:30:02] [WARNING] ‚ö†Ô∏è     - compta_88888 (n√©cessite un import --full pour historique complet)
[2025-01-25 14:30:02] [WARNING] ‚ö†Ô∏è     - compta_77777 (n√©cessite un import --full pour historique complet)
[2025-01-25 14:30:02] [INFO] ‚ÑπÔ∏è  Import incr√©mental limit√© √† 3497 dossiers connus
```

---

#### 3. **V√©rification du m√©canisme d'import incr√©mental** (PARTIELLEMENT VALID√â)

**Prompt de v√©rification** :
> "Analyser le m√©canisme d'import incr√©mental dans `02_import_raw_compta.sh` (lignes 221-255) pour v√©rifier :
>
> 1. **Pas de perte de donn√©es** :
>    - Les √©critures modifi√©es entre deux imports sont bien captur√©es ?
>    - Le filtre `WHERE t.DATE_FIELD > '$LAST_SYNC'` est-il strict ou inclusif ?
>    - Que se passe-t-il si une √©criture est modifi√©e **pendant** l'import ?
>
> 2. **Pas de doublons** :
>    - La clause `ON DUPLICATE KEY UPDATE` fonctionne-t-elle correctement ?
>    - Les cl√©s primaires `(dossier_code, CODE, ANNEE)` sont-elles suffisantes ?
>    - Les donn√©es de la table `compte` et `journal` (sans filtre date) peuvent-elles cr√©er des doublons ?
>
> 3. **Gestion des suppressions** :
>    - Si une √©criture est **supprim√©e** dans ACD, elle reste dans `raw_acd` ?
>    - Faut-il ajouter un soft delete ou un m√©canisme de purge ?
>
> 4. **Tests recommand√©s** :
>    - Cr√©er une base de test `compta_test` avec quelques √©critures
>    - Lancer un import full
>    - Modifier/ajouter/supprimer des √©critures dans `compta_test`
>    - Lancer un import incr√©mental
>    - V√©rifier que les modifications sont bien refl√©t√©es dans `raw_acd`"

**Actions sugg√©r√©es** :
- Impl√©menter des tests unitaires pour l'import incr√©mental
- Ajouter un syst√®me de logs d√©taill√© (nombre de lignes ins√©r√©es/mises √† jour par base)
- Cr√©er une table `sync_audit` pour tracer tous les imports :
  ```sql
  CREATE TABLE raw_acd.sync_audit (
      id BIGINT AUTO_INCREMENT PRIMARY KEY,
      table_name VARCHAR(50),
      dossier_code VARCHAR(20),
      sync_date DATETIME,
      sync_type ENUM('full', 'incremental'),
      rows_inserted INT,
      rows_updated INT,
      duration_sec INT,
      status VARCHAR(20)
  );
  ```

---

#### 3. **Optimisation de l'import pour 3500 bases**

**Probl√®mes actuels** :
- Import s√©quentiel = ~19 heures
- Pas de monitoring en temps r√©el du transfert r√©seau
- Pas de v√©rification de l'espace disque avant import

**Solutions** :
- ‚úÖ **FAIT** : Barre de progression avec timestamps toutes les 10 bases
- **TODO** : Ajouter une v√©rification d'espace disque avant `--full`
- **TODO** : Impl√©menter un syst√®me de reprise en cas d'erreur (checkpoint)
- **TODO** : Ajouter des statistiques de transfert r√©seau (MB transf√©r√©s par base)

---

## üî¨ Optimisations avanc√©es (non impl√©ment√©es)

Cette section documente des optimisations qui n'ont **pas √©t√© impl√©ment√©es** mais qui pourraient √™tre utiles dans des cas sp√©cifiques.

### 1. Mode staging avec swap atomique

**Probl√®me** : En cas de crash durant l'import, les donn√©es peuvent √™tre partiellement import√©es, cr√©ant un √©tat incoh√©rent.

**Solution** :
```bash
# 1. Import dans des tables temporaires
LOAD DATA LOCAL INFILE '/tmp/data.tsv'
REPLACE INTO TABLE raw_acd.histo_ligne_ecriture_tmp ...

# 2. V√©rifier la coh√©rence des donn√©es
if [ validation OK ]; then
    # 3. Swap atomique
    RENAME TABLE
        histo_ligne_ecriture TO histo_ligne_ecriture_old,
        histo_ligne_ecriture_tmp TO histo_ligne_ecriture;

    DROP TABLE histo_ligne_ecriture_old;
fi
```

**Avantages** :
- ‚úÖ Rollback automatique en cas d'erreur
- ‚úÖ Pas d'√©tat interm√©diaire incoh√©rent
- ‚úÖ Les utilisateurs voient toujours des donn√©es compl√®tes

**Inconv√©nients** :
- ‚ùå Complexit√© √©lev√©e de mise en ≈ìuvre
- ‚ùå Double espace disque n√©cessaire pendant l'import
- ‚ùå N√©cessite de dupliquer toutes les structures (tables, index, partitions)

**Statut** : Non impl√©ment√©
- Gain marginal pour le contexte actuel (volum√©trie <200k lignes/dossier)
- M√©canisme de reprise apr√®s crash via `sync_tracking_by_dossier` suffit

---

### 2. Fallback pour requ√™tes lentes

**Probl√®me** : Les requ√™tes avec `WHERE EXISTS` sur les jointures `HE_CODE`/`ECR_CODE` peuvent √™tre lentes si les tables sources ACD n'ont pas d'index sur ces colonnes.

**Exemple de requ√™te potentiellement lente** :
```sql
SELECT * FROM ligne_ecriture
WHERE EXISTS (
    SELECT 1 FROM ecriture e
    WHERE e.ECR_CODE = ligne_ecriture.ECR_CODE
    AND e.ECR_DATE_SAI > '2025-01-01'
)
```

**Solution** :
```bash
# Timeout de 30 secondes sur l'extraction
timeout 30s $MYSQL -h "$ACD_HOST" ... || {
    log "WARNING" "Requ√™te lente d√©tect√©e pour $DB.$TABLE, fallback import complet"
    WHERE_CLAUSE=""  # Import complet pour cette table
}
```

**Avantages** :
- ‚úÖ √âvite de bloquer l'import sur une base probl√©matique
- ‚úÖ Garantit la progression m√™me en cas de probl√®me de performance

**Inconv√©nients** :
- ‚ùå Perte de l'optimisation incr√©mentale pour cette base
- ‚ùå Augmentation du temps d'import pour les bases concern√©es

**Statut** : Non impl√©ment√©
- Aucune compta ne d√©passe 200 000 lignes actuellement
- Performance acceptable dans tous les cas observ√©s
- √Ä consid√©rer si volum√©trie augmente significativement

---

### 3. Approche en 2 temps pour jointures

**Alternative aux `WHERE EXISTS`** si probl√®mes de performance :

```bash
# 1. R√©cup√©rer les codes d'√©critures modifi√©es
MODIFIED_ECR_CODES=$($MYSQL -h "$ACD_HOST" -N -e "
    SELECT GROUP_CONCAT(ECR_CODE)
    FROM \`$DB\`.ecriture
    WHERE ECR_DATE_SAI > '$SYNC_DATE'
")

# 2. Filtrer les lignes avec IN clause
WHERE_CLAUSE="WHERE ECR_CODE IN ($MODIFIED_ECR_CODES)"

# 3. Extraction avec filtre direct
$MYSQL -h "$ACD_HOST" -e "
    SELECT $SELECT_COLS
    FROM \`$DB\`.ligne_ecriture
    $WHERE_CLAUSE
"
```

**Avantages** :
- ‚úÖ Plus rapide si pas d'index sur colonne de jointure
- ‚úÖ Requ√™te plus simple sans sous-requ√™te corr√©l√©e
- ‚úÖ Peut b√©n√©ficier du cache MySQL

**Inconv√©nients** :
- ‚ùå Deux requ√™tes au lieu d'une (overhead r√©seau)
- ‚ùå Limite de taille pour la clause `IN` (~1000-10000 valeurs selon config MySQL)
- ‚ùå Peut √©chouer si trop d'√©critures modifi√©es

**Statut** : Non impl√©ment√©
- Approche actuelle avec `WHERE EXISTS` suffisante
- √Ä consid√©rer uniquement si probl√®mes de performance av√©r√©s
- N√©cessiterait gestion du chunking pour grandes volum√©tries

---

### 4. Compression r√©seau avanc√©e

**Probl√®me** : Le transfert r√©seau peut √™tre lent entre le serveur ACD et le serveur local, surtout pour les imports full.

**Solutions possibles** :
```bash
# Option 1 : SSH tunnel avec compression
ssh -C -L 3307:localhost:3306 user@acd-server

# Option 2 : Compression MySQL native (d√©j√† utilis√©e)
$MYSQL --compress -h "$ACD_HOST" ...

# Option 3 : Compression ZSTD (MySQL 8.0.18+)
$MYSQL --compression-algorithms=zstd -h "$ACD_HOST" ...
```

**Statut** : Partiellement impl√©ment√©
- ‚úÖ `--compress` d√©j√† utilis√© dans le script actuel
- ‚è≥ Compression ZSTD √† tester si disponible sur serveur source

---

### 5. Parall√©lisme intelligent par volum√©trie

**Probl√®me** : Toutes les bases sont trait√©es s√©quentiellement, m√™me si certaines sont tr√®s petites.

**Solution** :
```bash
# Trier les bases par volum√©trie estim√©e
SORTED_DATABASES=$(for DB in "${ELIGIBLE_DATABASES[@]}"; do
    SIZE=$($MYSQL -h "$ACD_HOST" -N -e "
        SELECT SUM(data_length)
        FROM information_schema.tables
        WHERE table_schema = '$DB'
        AND table_name IN ('ecriture', 'ligne_ecriture')
    ")
    echo "$SIZE|$DB"
done | sort -rn | cut -d'|' -f2)

# Traiter les grosses bases en premier (optimisation du temps total)
```

**Avantages** :
- ‚úÖ Meilleure estimation du temps restant
- ‚úÖ √âchecs pr√©coces sur bases probl√©matiques

**Inconv√©nients** :
- ‚ùå Requ√™te suppl√©mentaire par base avant import
- ‚ùå Complexit√© accrue

**Statut** : Non impl√©ment√©
- Source ACD √† 1 CPU : parall√©lisme limit√© de toute fa√ßon
- Traitement s√©quentiel plus simple et pr√©visible

---

## üìû Support

Pour toute question, v√©rifiez :
1. Les logs dans `logs/pipeline_YYYYMMDD.log`
2. Les statistiques : `bash bash/raw/02c_cleanup_acd.sh --stats`
3. La table de tracking : `SELECT * FROM raw_acd.sync_tracking;`
4. La table de tracking par dossier : `SELECT * FROM raw_acd.sync_tracking_by_dossier LIMIT 20;`
